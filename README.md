# Using Large Language Model as Localize Hate Speech Detector

## Project structure
There are two main parts of this project: dataset gathering and classifier training.
```
./
├── classifier/         --- LoRA tuning & Inference on Taiwan-LLM-7B 
├── result/             --- Result files generated by classifier.
└── data_process/       --- Programs of processing dataset **TWHS**
    └──  crawler/       --- Python crawler to fetch post, comments
        ├── ptt         --- HTMLSession will do the job
        ├── Line_Today  --- Using selemium for the job
        ├── Youtube
        └── Bahamut     --- Using selemium for the job, require cookies
```
Besides, there is no scripts. All training & inference are build by executing.

## Process
### Dataset 
First, we fetch the information we need on each website by running crawler. 
After this, we run preprocess to make dataset's negative and postive rate at 1:2.
### Human labeled
We labeled the data on docanno.
### Auto label classifier
In our first stage, we LoRA-tuning Taiwan-LLM-7B on our dataset of human labeled.
### TWHS
Execute same rule of preprocessing, we can get our TWHS dataset directly.
### Classifier
In our second stage, we LoRA-tuning Taiwan-LLM-7B on TWHS dataset.
### TWHS-E
We use same method to collected dataset TWHS-E and selected the fuzzy speech by human.
### Evaluation
By running evaluation, we can get the metrics and seeable result.

TODO: 把爬蟲的東西丟進/data_process/crawler
TODO: 把Evaluation程式傳上來